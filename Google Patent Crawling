import requests
from bs4 import BeautifulSoup

#Creating URL

search = input('What do you want to Search? : ')
inventor = input('Inventor? : ')
inventor2 = input('Another Inventor? IF no additional Inventor, Type "no" : ')
assignee = input('Assignee? : ')
assignee2 = input('Another Assignee? IF no additional Assignee, Type "no" : ')
prdate_start = input('Start Date? (ex. 20000101) : ')
prdate_end = input('End Date? (ex. 20001231) : ')
synonym = input('Synonym? IF no synonym, Type "no" : ')

if inventor2 == 'no':
    ivt = '&inventor='+inventor
else:
    ivt = '&inventor='+inventor+','+inventor2

if assignee2 == 'no':
    asn = '&assignee='+assignee
else:
    asn = '&assignee='+assignee+','+assignee2


prdate = '&before=priority:'+prdate_start+'&after=priority:'+prdate_end
LIMIT = 50
URL = f'http://patents.google.com/?q='+search+','+synonym+ivt+asn+prdate+'&page={LIMIT}'


#Find Page

def get_last_page():
    gp_result = requests.get(URL)

    gp_soup = BeautifulSoup(gp_result.text, 'html.parser')

    pagination = gp_soup.find('div', {'class': 'style-scope search-results'})

    links = pagination.find_all('a')

    pages = []

    for link in links[:-1]:
        pages.append(int(link.string))

    max_page = pages[-1]
    return max_page


def extract_pt(html):
    title = html.find('div', {'class': 'result-title style-scope search-result-item'}).find('a')['title']

    company = html.find('span', {'class':'style-scope raw-html'})
    company_anchor = company.find('a')

    if company_anchor is not None:
        company = str(company_anchor.string)
    else:
        company = str(company.string)
    company = company.strip()
    location = html.find('div', {'class': 'recJobLoc'})['data-rc-loc']
    job_id = html['data-jk']

    return {'title': title, 'company': company, 'location': location,
            'link': f'https://www.indeed.com/viewjob?jk={job_id}'}

#Extract information

def extract_pts(last_page):
    pts = []
    for page in range(last_page):
        print(f'Scrapping Page {page}')
        result = requests.get(f'{URL}&start={page * LIMIT}')
        soup = BeautifulSoup(result.text, 'html.parser')
        results = soup.find_all('div', {'class': 'jobsearch-SerpJobCard'})
        for result in results:
            pt = extract_pt(result)
            pts.append(pt)
    return pts


def get_pts():
    last_page = get_last_page()

    pts = extract_pts(last_page)
    return pts

def save_to_file(jobs):
  file = open('pts.csv', mode='w')
  writer = csv.writer(file)
  writer.writerow(['title', 'company', 'location', 'link'])
  for job in jobs:
    writer.writerow(list(job.values()))
  return

pts = get_pts()
save_to_file(pts)
