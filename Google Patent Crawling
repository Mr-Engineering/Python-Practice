import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_driver_path = 'D:/Study/Python_Study/Drivers/chromedriver.exe'
driver = webdriver.Chrome(options=chrome_options, executable_path=chrome_driver_path)

#Creating URL

search = input('What do you want to Search? : ')
inventor = input('Inventor? : ')
inventor2 = input('Another Inventor? IF no additional Inventor, Type "no" : ')
assignee = input('Assignee? : ')
assignee2 = input('Another Assignee? IF no additional Assignee, Type "no" : ')
prdate_start = input('Start Date? (ex. 20000101) : ')
prdate_end = input('End Date? (ex. 20001231) : ')
synonym = input('Synonym? IF no synonym, Type "no" : ')

if inventor2 == 'no':
    ivt = '&inventor='+inventor
else:
    ivt = '&inventor='+inventor+','+inventor2

if assignee2 == 'no':
    asn = '&assignee='+assignee
else:
    asn = '&assignee='+assignee+','+assignee2


prdate = '&before=priority:'+prdate_end+'&after=priority:'+prdate_start
LIMIT = 50
URL = f'http://patents.google.com/?q='+search+','+synonym+ivt+asn+prdate+'&page={LIMIT}'

driver.get(URL)
html = driver.page_source

gp_soup = BeautifulSoup(html, 'html.parser')
print(gp_soup)
pagination = gp_soup.find('div', {'id': 'pagingAndInfo'})

print(pagination)

#Find Page

def get_last_page():

    gp_soup = BeautifulSoup(html, 'html.parser')

    pagination = gp_soup.find_all('div', {'class': 'style-scope search-paging'})

    links = pagination.find('a')

    for link in links[:-1]:
        pages.append(int(link.string))

    max_page = pages[1]
    return max_page


def extract_pt(html):
    title = html.find('div', {'class': 'result-title style-scope search-result-item'}).find('a')['title']

    company = html.find('span', {'class':'style-scope raw-html'})
    company_anchor = company.find('a')

    if company_anchor is not None:
        company = str(company_anchor.string)
    else:
        company = str(company.string)
    company = company.strip()
    location = html.find('div', {'class': 'recJobLoc'})['data-rc-loc']
    job_id = html['data-jk']

    return {'title': title, 'company': company, 'location': location,
            'link': f'https://www.indeed.com/viewjob?jk={job_id}'}

#Extract information

def extract_pts(last_page):
    pts = []
    for page in range(last_page):
        print(f'Scrapping Page {page}')
        result = requests.get(f'{URL}&start={page * LIMIT}')
        soup = BeautifulSoup(result.text, 'html.parser')
        results = soup.find_all('div', {'class': 'jobsearch-SerpJobCard'})
        for result in results:
            pt = extract_pt(result)
            pts.append(pt)
    return pts


def get_pts():
    last_page = get_last_page()

    pts = extract_pts(last_page)
    return pts

def save_to_file(jobs):
  file = open('pts.csv', mode='w')
  writer = csv.writer(file)
  writer.writerow(['title', 'company', 'location', 'link'])
  for job in jobs:
    writer.writerow(list(job.values()))
  return

pts = get_pts()
save_to_file(pts)
